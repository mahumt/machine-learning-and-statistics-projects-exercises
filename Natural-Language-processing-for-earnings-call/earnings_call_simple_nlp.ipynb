{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2018/02/the-different-methods-deal-text-data-predictive-python/ <br>\n",
    "https://github.com/yyeung-lam/earnings-call-text-mining <br>\n",
    "https://towardsdatascience.com/text-mining-and-classification-on-earnings-call-transcripts-e63d9856d7d8 "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Install spacy, textacy and nltk. <br>\n",
    "Install spacy module through command  python -m spacy download en_core_web_sm . <br>\n",
    "If the popup window remain, manually close them or the programm will not advance. <br>\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import pandas as pd\n",
    "import textacy, spacy\n",
    "import tkinter as tk\n",
    "from tkinter import filedialog\n",
    "from tkinter import *\n",
    "import en_core_web_sm\n",
    "from textblob import TextBlob\n",
    "from textacy import TextStats\n",
    "import sys, os, re"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 8,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "C:/Users/Mahum/Desktop/ALL/PYTHON/NLP/default/raw\n"
     ]
    }
   ],
   "source": [
    "def browse_button():\n",
    "    # Allow user to select a directory and store it in global var\n",
    "    # called folder_path\n",
    "    \n",
    "    global folder_path\n",
    "    global filename\n",
    "    filename = filedialog.askdirectory()\n",
    "    folder_path.set(filename)\n",
    "    print(filename)\n",
    "    sourcePath = folder_path.get()\n",
    "    os.chdir(sourcePath)  # Provide the path here\n",
    "\n",
    "root = Tk()\n",
    "\n",
    "folder_path = StringVar()\n",
    "\n",
    "lbl1 = Label(master=root,textvariable=folder_path)\n",
    "lbl1.grid(row=0, column=1)\n",
    "\n",
    "buttonBrowse = Button(text=\"        Set directory where the raw files are located        \", \n",
    "         \n",
    "                      command = browse_button, bg = 'green',\n",
    "                      fg='white', font = ('helvetica', 12, 'bold'))\n",
    "buttonBrowse.grid(row=2, column=1)\n",
    "mainloop()"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Step One: Data Cleaning and Preprocessing\n"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Clean the text and save it to another folder"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [],
   "source": [
    "nlp = en_core_web_sm.load()\n",
    "#nlp = spacy.load(\"en_core_web_sm\")\n",
    "\n",
    "\n",
    "def tokenize(text):\n",
    "    paragraphs = text.split('\\n')\n",
    "    processed_text = \"\"\n",
    "    for p in paragraphs:\n",
    "        if len(p) < 15:\n",
    "            continue\n",
    "        doc = nlp(p)\n",
    "        for token in doc:\n",
    "            if not token.is_punct:\n",
    "                replaced_tok = token.lemma_\n",
    "                if token.lemma_ == '-PRON-':\n",
    "                    replaced_tok = 'it'\n",
    "                processed_text += (replaced_tok + \" \")\n",
    "            else:\n",
    "                processed_text += (token.text + \" \")\n",
    "    processed_text = re.sub(r\"(good morning( everyone)*)\", \"\", processed_text)\n",
    "    processed_text = re.sub(r\"(thank you)\", \"\", processed_text)\n",
    "    processed_text = re.sub(r\"(thank(s)*)\", \"\", processed_text)\n",
    "    return processed_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read the data"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 10,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Processing (CYH).txt\n",
      "Processing (FE).txt\n",
      "Processing (FMSA).txt\n",
      "Processing (IHRT).txt\n",
      "Processing (MFRM).txt\n"
     ]
    }
   ],
   "source": [
    "def read_file(directory):\n",
    "    all_files = os.listdir(directory)\n",
    "    global text_dir\n",
    "    for text_dir in all_files:\n",
    "        if  text_dir.startswith(\".\"):\n",
    "            continue\n",
    "        if os.path.isdir(text_dir):\n",
    "            continue\n",
    "        with open (directory + '/'  + text_dir, 'r', encoding='utf-8') as f: #change raw to the last folder\n",
    "            text = f.read()\n",
    "        print(\"Processing\", text_dir)\n",
    "        with open(directory + '/' + text_dir, 'w', encoding='utf-8') as p:\n",
    "            p.write(tokenize(text))   \n",
    "        \n",
    "if __name__ == '__main__':\n",
    "    directory = sys.argv[1]\n",
    "    \n",
    "read_file(filename)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Load the dataset"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 12,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Total doc: 5\n"
     ]
    }
   ],
   "source": [
    "def load_data(data, label):\n",
    "    for file in os.listdir(label):\n",
    "        if not file.startswith('.'):\n",
    "            file_path =  label + '/' +  file \n",
    "            with open(file_path, 'r', encoding='utf-8') as f:\n",
    "                text = f.read()\n",
    "                data[text] = label\n",
    "                \n",
    "df = {}\n",
    "load_data(df, filename)\n",
    "\n",
    "Num = len(df)\n",
    "print(\"Total doc:\", Num)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Exploratory sentiment analysis"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/html": [
       "<div>\n",
       "<style scoped>\n",
       "    .dataframe tbody tr th:only-of-type {\n",
       "        vertical-align: middle;\n",
       "    }\n",
       "\n",
       "    .dataframe tbody tr th {\n",
       "        vertical-align: top;\n",
       "    }\n",
       "\n",
       "    .dataframe thead th {\n",
       "        text-align: right;\n",
       "    }\n",
       "</style>\n",
       "<table border=\"1\" class=\"dataframe\">\n",
       "  <thead>\n",
       "    <tr style=\"text-align: right;\">\n",
       "      <th></th>\n",
       "      <th>n_words</th>\n",
       "      <th>n_sents</th>\n",
       "      <th>n_long_words</th>\n",
       "      <th>ARI</th>\n",
       "    </tr>\n",
       "  </thead>\n",
       "  <tbody>\n",
       "    <tr>\n",
       "      <th>count</th>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "      <td>5.000000</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>mean</th>\n",
       "      <td>9205.000000</td>\n",
       "      <td>539.200000</td>\n",
       "      <td>1776.400000</td>\n",
       "      <td>7.550944</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>std</th>\n",
       "      <td>1863.216708</td>\n",
       "      <td>86.053472</td>\n",
       "      <td>238.365895</td>\n",
       "      <td>0.447144</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>min</th>\n",
       "      <td>8012.000000</td>\n",
       "      <td>475.000000</td>\n",
       "      <td>1526.000000</td>\n",
       "      <td>6.823956</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>25%</th>\n",
       "      <td>8113.000000</td>\n",
       "      <td>494.000000</td>\n",
       "      <td>1681.000000</td>\n",
       "      <td>7.483967</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>50%</th>\n",
       "      <td>8142.000000</td>\n",
       "      <td>509.000000</td>\n",
       "      <td>1686.000000</td>\n",
       "      <td>7.624507</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>75%</th>\n",
       "      <td>9373.000000</td>\n",
       "      <td>529.000000</td>\n",
       "      <td>1833.000000</td>\n",
       "      <td>7.880709</td>\n",
       "    </tr>\n",
       "    <tr>\n",
       "      <th>max</th>\n",
       "      <td>12385.000000</td>\n",
       "      <td>689.000000</td>\n",
       "      <td>2156.000000</td>\n",
       "      <td>7.941580</td>\n",
       "    </tr>\n",
       "  </tbody>\n",
       "</table>\n",
       "</div>"
      ],
      "text/plain": [
       "            n_words     n_sents  n_long_words       ARI\n",
       "count      5.000000    5.000000      5.000000  5.000000\n",
       "mean    9205.000000  539.200000   1776.400000  7.550944\n",
       "std     1863.216708   86.053472    238.365895  0.447144\n",
       "min     8012.000000  475.000000   1526.000000  6.823956\n",
       "25%     8113.000000  494.000000   1681.000000  7.483967\n",
       "50%     8142.000000  509.000000   1686.000000  7.624507\n",
       "75%     9373.000000  529.000000   1833.000000  7.880709\n",
       "max    12385.000000  689.000000   2156.000000  7.941580"
      ]
     },
     "execution_count": 13,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#Analyze a Doc\n",
    "corpus = textacy.Corpus(nlp, df)\n",
    "\n",
    "stat = pd.DataFrame(columns = ['n_words', 'n_sents', 'n_long_words', 'ARI'])\n",
    "doc_id = 0\n",
    "for doc in corpus:\n",
    "    ts = TextStats(doc)\n",
    "    stat.loc[doc_id] = [ts.n_words, ts.n_sents, ts.n_long_words, ts.automated_readability_index]\n",
    "    doc_id += 1\n",
    "    \n",
    "stat.describe()"
   ]
  },
  {
   "cell_type": "raw",
   "metadata": {},
   "source": [
    "# TextBlob Sentiment Analysis\n",
    "#import nltk\n",
    "#nltk.download('punkt')\n",
    "\n",
    "tb = TextBlob(default_corpus[0].text)\n",
    "\n",
    "for sent in tb.sentences:\n",
    "    print(sent.sentiment.polarity)\n",
    "print(len(tb.sentences))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "def sentiment_by_sentence(text, doc_id, default):\n",
    "    sentiment_df = pd.DataFrame(columns = [\"default\", \"doc_id\", \"polarity\", \"subjectivity\", \"progress\"])\n",
    "    tb = TextBlob(text)\n",
    "    sent_id = 0\n",
    "    num_sents = len(tb.sentences)\n",
    "    for sent in tb.sentences:\n",
    "        global sentiment\n",
    "        sentiment = sent.sentiment\n",
    "        sentiment_df.loc[sent_id] = [default, doc_id, sentiment.polarity, sentiment.subjectivity, round((sent_id+1)/num_sents,4)]\n",
    "        sent_id += 1\n",
    "    return sentiment_df\n",
    "\n",
    "df_sentiment = pd.DataFrame(columns = [\"default\", \"doc_id\", \"polarity\", \"subjectivity\", \"progress\"])\n",
    "doc_id = 1\n",
    "for doc in df:\n",
    "    df_sentiment = pd.concat([df_sentiment, sentiment_by_sentence(doc, doc_id, True)])\n",
    "    doc_id += 1\n",
    "    \n",
    "\n",
    "df_sentiment.to_csv(\"SentimentData.csv\") #saves it in the driectory"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(   Unnamed: 0  default  doc_id  polarity  subjectivity  progress\n",
       " 0           0     True       1  0.125000      0.125000    0.0025\n",
       " 1           1     True       1  0.178571      0.357143    0.0050\n",
       " 2           2     True       1  0.000000      0.000000    0.0075\n",
       " 3           3     True       1  0.000000      0.062500    0.0100\n",
       " 4           4     True       1  0.000000      0.000000    0.0125,\n",
       "       Unnamed: 0  default  doc_id  polarity  subjectivity  progress\n",
       " 2101         475     True       5  0.250000      0.750000    0.9917\n",
       " 2102         476     True       5  0.000000      0.000000    0.9938\n",
       " 2103         477     True       5 -0.002083      0.852083    0.9958\n",
       " 2104         478     True       5  0.004416      0.563052    0.9979\n",
       " 2105         479     True       5  0.037857      0.469286    1.0000)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "SentimentData = pd.read_csv(\"SentimentData.csv\")\n",
    "SentimentData.head(), SentimentData.tail()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
